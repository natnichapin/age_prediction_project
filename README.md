1. Dataset Selection: Each group will work with a dataset at
https://archive.ics.uci.edu/datasets, or https://www.kaggle.com/datasets.
2. Data Exploration:
• Explore the dataset to understand attributes, data types, and patterns.
• Create visualizations (e.g., histograms, scatter plots) to gain insights
into data distribution.
3. Data Preprocessing:
• Handle missing values using techniques such as imputation or
removal.
• Normalize or standardize data if needed for consistent scaling.
4. Feature Selection: (Data correlation, RFE, Decision Tree, or PCA)
5. Hierarchical Clustering:
• Use a Python library (e.g., scikit-learn) for hierarchical clustering:
• Choose a linkage method (e.g., complete, average, single).
• Compute distance matrix with appropriate metric (e.g., Euclidean,
Manhattan).
• Perform hierarchical clustering using library functions.
• Visualize dendrograms to determine optimal clusters.
6. K-means Clustering:
• Use a Python library (e.g., scikit-learn) for K-means clustering:
• Choose number of clusters (k) based on insights or finding best k.
• Perform K-means clustering using library functions.
• Visualize clustering results if possible.
7. Interpretation and Analysis:
• Interpret clustering results from both methods.
• Analyze cluster characteristics, central tendencies, and variations.
8. Cluster Validation:
• Explore basic cluster validation metrics (e.g., silhouette score).
• Discuss how these metrics assess clustering quality.
9. Comparison and Discussion:
• Prepare presentation summarizing findings from both clustering
methods.
• Discuss differences, advantages, and limitations of hierarchical and
K-means clustering.
10. Conclusion:
• Conclude by discussing practical applications of hierarchical and K-
means clustering.
