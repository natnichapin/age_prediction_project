# -*- coding: utf-8 -*-
"""Practice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15zqfwRccC9Dv8HwQJxvM7HOb18T8KQXb
"""

import pandas as pd # Assuming you'll use Pandas for data manipulation
import os # For handling file paths

data_folder = "sample_data"
file_name = "age_prediction.csv" # Replace with the actual file name
file_path = os.path.join(data_folder, file_name)


df = pd.read_csv(file_path) # Assuming it's a CSV file

print("First 5 records:", df.head())

percent_missing = round (100*( df . isnull ().sum ())/len( df ),2)
percent_missing

# Check for missing values
print(df.isnull().sum())

# Drop rows with missing values (if necessary)
penguins = df.dropna()

# prompt: pull .data and .column from df  which data from df = pd.read_csv(file_path)
import pandas as pd
df_pandas = pd.DataFrame(df)
data = df_pandas.values
column = df_pandas.columns
my_df = pd.DataFrame(data=data, columns=column)

my_df

cleaned_data = my_df.drop (['Age_group'], axis =1)
cleaned_data . info ()

# columns_to_drop = [col for col in my_df.columns if col == 'Age_group']
# cleaned_data = my_df.drop(columns=columns_to_drop, axis=1)
# cleaned_data . info ()

cleaned_data

# Import packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Convert the dataset into a pandas DataFrame

# Compute the correlation matrix
f, ax = plt.subplots(figsize=(10, 8))
corr = cleaned_data.corr(method="pearson")  # Calculate correlations between features

# Plot the heatmap
sns.heatmap(corr, mask=np.zeros_like(corr, dtype=bool),
            cmap=sns.diverging_palette(220, 10, as_cmap=True),
            square=True, ax=ax)

plt.show()

# 3. ตรวจสอบ Outliers ที่อาจมีผลต่อค่า Correlation
#  ถ้ากระจายเป็นเส้นตรง → Pearson น่าจะถูกต้อง (0.78)
#  ถ้าเป็นแนวโค้ง หรือกระจายแบบไม่มีรูปแบบ → Spearman น่าจะถูกต้องกว่า (0.21)
sns.scatterplot(x=df["Blood Glucose after fasting"], y=df["Blood Insulin Levels"])
plt.show()

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans


# Select relevant numerical features for clustering
features = ["Age", "Body Mass Index", "Respondent's Oral", "Blood Insulin Levels","Blood Glucose after fasting"]
X = cleaned_data[features]

# Standardize the data for better clustering performance
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.datasets import make_blobs

k_values = range(2, 10)
silhouette_scores = []

# Compute Silhouette Score for each k
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, cluster_labels)
    silhouette_scores.append(score)

# Plot the results
plt.figure(figsize=(8, 5))
plt.plot(k_values, silhouette_scores, marker='o', linestyle='dashed')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Silhouette Score")
plt.title("Silhouette Score vs. Number of Clusters")
plt.show()

wcss = []
k_values = range(1, 11)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plot Elbow Method
plt.plot(k_values, wcss, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.title('Elbow Method for Optimal k')
plt.show()

from sklearn.cluster import KMeans

# Set K-Means with k=3
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
cleaned_data['Cluster'] = kmeans.fit_predict(X_scaled)  # Assign clusters to the data

# View first few rows with cluster assignments
# print(cleaned_data.head())
centroids = kmeans.cluster_centers_
print("Cluster Centroids (Scaled):", centroids)

print(cleaned_data['Cluster'].value_counts())

print(cleaned_data.groupby('Cluster').mean())

import matplotlib.pyplot as plt
import seaborn as sns
# If data has only 2 numerical features
# plt.figure(figsize=(8, 5))
# sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=cleaned_data['Cluster'], palette='viridis')
# plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X', label='Centroids')
# plt.title("K-Means Clustering (k=3)")
# plt.legend()
# plt.show()
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(8, 5))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=cleaned_data['Cluster'], palette='viridis')
plt.scatter(pca.transform(centroids)[:, 0], pca.transform(centroids)[:, 1],
            s=300, c='red', marker='X', label='Centroids')
plt.title("K-Means Clustering (PCA Reduced, k=3)")
plt.legend()
plt.show()

# 4. Interpret the Clusters
# Look at cluster size, means, and characteristics.
# Check if cluster labels make sense (e.g., High, Medium, Low groups).
# Use the cluster labels for business decisions, anomaly detection, or feature engineering.

import numpy as np
import matplotlib . pyplot as plt
import seaborn as sns
from scipy . cluster . hierarchy import dendrogram , linkage , fcluster

# Generate random data for demonstration
# np. random . seed (123 )
# data = np. random . rand (10 , 2)
# linked = linkage (data , 'single ')

features = ["Age", "Body Mass Index", "Respondent's Oral", "Blood Insulin Levels","Blood Glucose after fasting"]
X = cleaned_data[features]

# Perform hierarchical clustering
linked = linkage(X, 'complete')

num_rows = len(cleaned_data)  # Get the actual number of rows

plt . figure ( figsize =(10 , 5))
dendrogram (linked ,orientation ='top',labels = range (1, num_rows+1))
plt.title ('Dendrogram')
plt.xlabel ('Sample Index')
plt.ylabel ('Distance')
plt.show ()

